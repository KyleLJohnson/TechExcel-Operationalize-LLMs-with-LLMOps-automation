---
title: '1. Evalute your chat flow'
layout: default
nav_order: 1
parent: 'Exercise 03: Evaluating and Deploying LLMs'
---

# Task 01 - Use AzureAI Studio Playground and create an AI Project and establish AI Hub resources

## Description

In this task, you will create and configure a Azure AI Studio project and define the system resources that will be used for the Hub.

## Success Criteria

* Verify each of the conditions with solution values

## Solution

<details markdown="block">
<summary>Expand this section to view the solution</summary>

##### 1) Evaluate your Chat flow

1. Go to your browser and type: https://ai.azure.com

2. Select the project created earlier and choose the **Prompt flow** item in the **Tools** section of the **Build** tab.

###### 1.1) Prepare you chat flow for evaluation

For the RAG flow that you created earlier to be evaluated, you must include additional information to the output node of this flow, specifically the context used to generate the answer.

This information will be used by the Evaluation Flow. To do this, just follow these steps:

1. In the Flows section of **Prompt Flow**, open the `Multi-Round Q&A on Your Data` flow that you created in the previous exercise. This will be the flow we use for evaluation.

![LLMOps Workshop](images/lab3grab1.png)

2. Create a new output named `documents` in the Outputs node. This output will represent the documents that were retrieved in the `lookup` node and subsequently formatted in the `generate_prompt_context` node.

3. Assign the output of the `generate_prompt_context` node to the `documents` output, as shown in the image below.

![LLMOps Workshop](images/lab3grab2.png)

4. Click **Save** before moving to the next section.

###### 1.2) Create your evaluation flows

1. Still in the **Prompt flow** item in the **Tools** section of the **Build** tab, click on the blue **Create** button.

![LLMOps Workshop](images/lab3grab3.png)

2. Select the **Evaluation Flow** filter and click on **Clone** on the **QnA Groundedness Evaluation** card.

![LLMOps Workshop](images/lab3grab4.png)

3. Click on the other **Clone** button to create a copy of the flow.

![LLMOps Workshop](images/lab3grab5.png)

4. A flow will be created with the following structure:

![LLMOps Workshop](images/lab3grab6.png)

5. Update the `Connection` field to point to a gpt-4 deployment in `groundedness_score` node also update max_tokens to `1000` as shown in the next figure.  
   
![LLMOps Workshop](images/lab3grab7.png)

6. After updating the connection information, click on **Save** in the evaluation flow and navigate to the Flows section in **Prompt Flow** item.

7. Now, you will repeat the same steps described so far in this **section 1.2** to create **two** additional evaluation flows, one `QnA Relevance Evaluation` and another `QnA GPT Similarity Evaluation`. The two images below show where these flows are in the prompt flow gallery.

> You will repeat **section 1.2** steps twice since you will need to create two additional evaluation flows.

> Note that the LLM nodes, where you will set the Azure OpenAI connection for each flow, have slightly different names: **relevance_score** and **similarity_score**, respectively.

QnA Relevance Evaluation:

![LLMOps Workshop](images/lab3grab8.png)


QnA GPT Similarity Evaluation:

![LLMOps Workshop](images/lab3grab9.png)


###### 1.3) Run the evaluation

In the Flows section of **Prompt Flow**, open the `Multi-Round Q&A on Your Data` flow that you created in the previous exercise. This will be the flow we use for evaluation.

1. Start the automatic runtime by selecting **Start compute session**. The runtime will be useful for you to work with the flow moving forward.

![LLMOps Workshop](images/lab3grab10.png)

2. Now select the **Custom evaluation** option in the Evaluate menu.

![LLMOps Workshop](images/lab3grab11.png)

3. In the `Prompt_variants` option, select the option to run only **two variants** to avoid reaching your GPT-4 model quota limit, as shown in the example image below, click **Next**.

![LLMOps Workshop](images/lab3grab12.png)

4. Select **Add new data**.

![LLMOps Workshop](images/lab3grab13.png)

5. Upload the file data.csv inside the lesson_03 folder.

![LLMOps Workshop](images/lab3grab14.png)

6. After clicking on **Add**  proceed to map the input fields as shown below: 

![LLMOps Workshop](images/lab3grab15.png)

7. Select the three evaluation flows you just created.

![LLMOps Workshop](images/lab3grab16.png)

8. Great job so far! Now, let's move on to the next step. Click on **Next** to set up the `question`, `context`, `ground_truth` and `answer` fields for each evaluation flow. You can see how to do this in the three images below.

> **Note:** Please take a moment to ensure you've selected the correct value. It's crucial for accurate metric calculation. Notice that the default values initially presented in the wizard are not the same as those indicated in the following images. Keep up the good work!

**QnA GPT Similarity Evaluation**

![LLMOps Workshop](images/lab3grab17.png)

**QnA Groundedness Evaluation**

![LLMOps Workshop](images/lab3grab18.png)

**QnA Relevance Evaluation**

![LLMOps Workshop](images/lab3grab19.png)

Click on **Submit** to start the evaluation.

![LLMOps Workshop](images/lab3grab20.png)

9. The evaluation process has started. To view all evaluations (one per variant), please navigate to the **Evaluation** section under the **Build** tab (If you see not started they are waiting on resources, they will say completed when finished).

![LLMOps Workshop](images/lab3grab21.png)

10. Upon selecting specific evaluation results, you will have the ability to view their detailed information.

11. You can also select **Switch to dashboard view** to access a dashboard that provides a tabular and visual comparison between the rounds of different variations, as shown in the following images.

*Table comparison*

![LLMOps Workshop](images/lab3grab22.png)

*Chart comparison*

![LLMOps Workshop](images/lab3grab23.png)

</details>
